---
layout: post
title: "The Use of Asymmetric Numeral System as an Accurate Replacement for Huffman Coding"
---
*Jarek Duda Khalid Tahboub Neeraj J. Gadgil Edward J. Delp*<br/>
https://arxiv.org/pdf/1901.04866.pdf


$s$ is a symbol

$x'$, $x$ different states of coded data, specifically $x'$ is the $x$ th appearance of symbol $s$.
 
$p_s$ probability associated with symbol $s$

## The coding equation
$C(s,x) = x'$, where $x' \approx x/p_s$

ANS tries to maintain this equation. Because information in a code $x$ is $log(x)$ bits (log base 2). Encoding a symbol $s$ with associated probability $p_s$ would result in the addition of $log(1/p_s)$ bits. So total information would be $log(x)+log(1/p_s) = log(x/p_s)$. So if the resultant code is x/p_s, this would be optimal.

For example in a binary symmetric number system (Figure), where $p_0 = 1/2$ and $p_1 = 1/2$,
The coding function would be
$C(0,x) = x/ (1/2)$, i.e., for 0, it is optimal.

$C(0,x) = x/ (1/2) + 1$, but for 1 it is approximately optimal.

In general, if the two uniform random variables parameters are not equal, we can write,
$C(s,x) = x/ p_s + s$. In the figure, $p_0 = 1/4$ and $p_1 = 3/4$

Now we define a function called the **symbol spread function**. 

$\bar{s}(x) = mod(x,b)$. 

It is a cyclic function. Here, $b = 2^n$ is the length of **symbol spread**.
In ANS the integer numbers are subdivide and assigned to the symbols based on their frequencies. Say symbol $s$ apears with $f_s$ frequency, then a fraction of the numbers in the symbol spread would be assigned to symbol so that $p_s \approx \frac{f_s}{2^n}$. 

For example, in the figure $\bar{s}(x) = mod(x,4) = (0123), \forall x \in \mathbb{N}$. That is the symbol spread is $4$, so we can set $f_0 = 1$ and $f_1 = 3$ to achieve the desired probabilities. And for this particular example (0123) is combined to (0111) since there are only two symbols.

So the coding equation becomes.

$C(s,x) = x \dfrac{2^n}{f_s} + s$

The coded states are integer, so rounding to the lowest integer,

$C(s,x) = \left \lfloor{x \dfrac{2^n}{f_s}}\right \rfloor + s$

To facilitate logical operations, the floored part can be rewritten as

$C(s,x) = \left \lfloor{\dfrac{x}{f_s}}\right \rfloor 2^n + mod(x,f_s) + s$

$\Rightarrow C(s,x) = \left \lfloor{\dfrac{x}{f_s}}\right \rfloor \ll n + (x \land f_s) + s$

The above formulas are for a special case where the second symbol has a relative shift of one integer. If the symbol spread function is (0111) as in figure x, the symbol 1 is shifted by one integer with respect to the previous symbol 0. If the spread function was (00111) for another particular arrangement of numbers (show figure) we would say the symbol 1 is shifted by two integer with respect to the previous symbol 0. So let's replace the $s$ term by the term offset $c_s$ which is the offset for symbol $s$ and given by $c_s = f_0 + f_1 ... + f_{s-1}$

$C(s,x) = \left \lfloor{\dfrac{x}{f_s}}\right \rfloor \ll n + (x \land f_s) + c_s$

**This is the final form of the coding function.**

For **the decoder equation** we have to simply invert the above equation

$x = f_s (x' \gg n) + (x' \land mask) -c_s $, where $mask = 2^n - 1 = (1 \ll n)-1$

and recoverd symbol $ s = \bar{s}(x') = mod(x', 2^n) = x \land mask$

So, the compact decoder function $D(x') = (s,x)$

## Renormalization

The encoded state $x'$ is shifted (left shift by $t_{prec}$ bits) to an arbitrary range $I = \{L, ... bL-1\}$ after decoding a symbol. Before encoding a symbol, the current state $x$ is adjusted so that an encode does not throw $x'$ out of $I$. This is done by removing $t_{prec}$ lowest significant bits from $x$ making sure it's in the range of $I_s = \{k*p_s ... b*k*p_s-1\}$






